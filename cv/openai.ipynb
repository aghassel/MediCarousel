{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import requests\n",
    "import os\n",
    "\n",
    "def get_marker_color(image_path):\n",
    "    def encode_image(img_path):\n",
    "        with open(img_path, \"rb\") as image_file:\n",
    "            return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "    # Getting the base64 string\n",
    "    base64_image = encode_image(image_path)\n",
    "\n",
    "    api_key = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4-vision-preview\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"In one word, tell me what is the color of the Crayola marker in this image?\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
    "                            \"detail\": \"low\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 50\n",
    "    }\n",
    "\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "    \n",
    "    # Extract color information from the response\n",
    "    try:\n",
    "        response_json = response.json()\n",
    "        color_info = response_json.get('choices', [])[0].get('message', {}).get('content', '')\n",
    "        return color_info\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blue.\n"
     ]
    }
   ],
   "source": [
    "image_path = \"blue.jpg\"\n",
    "print(get_marker_color(image_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Scalpel.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "audio_file = open(\"scalpel.m4a\", \"rb\")\n",
    "transcript = client.audio.transcriptions.create(\n",
    "  model=\"whisper-1\", \n",
    "  file=audio_file, \n",
    "  response_format=\"text\"\n",
    ")\n",
    "\n",
    "transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sF2H4WDk8FhLDAuc3nHrVpFY83RlAvJtb1lR5jFTBMK9yuRIq/65fw==\n"
     ]
    }
   ],
   "source": [
    "access_key = os.environ.get('PORCUPINE_API_KEY')\n",
    "\n",
    "print(access_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pvporcupine\n",
    "import pyaudio\n",
    "import struct\n",
    "import wave\n",
    "import openai\n",
    "\n",
    "access_key = os.environ.get('PORCUPINE_API_KEY')\n",
    "custom_keyword_path = 'hey-med-carousel_en_windows_v3_0_0.ppn'\n",
    "\n",
    "porcupine = pvporcupine.create(\n",
    "    access_key=access_key,\n",
    "    keyword_paths=[custom_keyword_path]\n",
    ")\n",
    "\n",
    "def record_audio(duration=2, filename=\"output.wav\"):\n",
    "    \"\"\"\n",
    "    Record audio from the default microphone for the given duration\n",
    "    and save it to the specified filename.\n",
    "    \"\"\"\n",
    "    pa = pyaudio.PyAudio()\n",
    "\n",
    "    stream = pa.open(format=pyaudio.paInt16, channels=1, rate=16000,\n",
    "                     input=True, frames_per_buffer=1024)\n",
    "\n",
    "    print(f\"Recording for {duration} seconds...\")\n",
    "\n",
    "    frames = []\n",
    "\n",
    "    for _ in range(0, int(16000 / 1024 * duration)):\n",
    "        data = stream.read(1024)\n",
    "        frames.append(data)\n",
    "\n",
    "    print(\"Recording finished.\")\n",
    "\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    pa.terminate()\n",
    "\n",
    "    with wave.open(filename, 'wb') as wf:\n",
    "        wf.setnchannels(1)\n",
    "        wf.setsampwidth(pa.get_sample_size(pyaudio.paInt16))\n",
    "        wf.setframerate(16000)\n",
    "        wf.writeframes(b''.join(frames))\n",
    "\n",
    "def transcribe_audio(filename):\n",
    "    \"\"\"\n",
    "    Transcribe the specified audio file using OpenAI's Whisper.\n",
    "    \"\"\"\n",
    "    client = openai.OpenAI()\n",
    "\n",
    "    with open(filename, \"rb\") as audio_file:\n",
    "        transcript = client.audio.transcriptions.create(\n",
    "            model=\"whisper-1\",\n",
    "            file=audio_file,\n",
    "            response_format=\"text\"\n",
    "        )\n",
    "\n",
    "    return transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening for the wake word...\n",
      "Wake word detected!\n",
      "Recording for 2 seconds...\n",
      "Recording finished.\n",
      "I like it. It's interesting. Interesting. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "pa = pyaudio.PyAudio()\n",
    "audio_stream = pa.open(rate=porcupine.sample_rate, channels=1,\n",
    "                       format=pyaudio.paInt16, input=True,\n",
    "                       frames_per_buffer=porcupine.frame_length)\n",
    "\n",
    "print(\"Listening for the wake word...\")\n",
    "\n",
    "while True:\n",
    "    pcm = audio_stream.read(porcupine.frame_length)\n",
    "    pcm = struct.unpack_from(\"h\" * porcupine.frame_length, pcm)\n",
    "\n",
    "    if porcupine.process(pcm) >= 0:\n",
    "        print(\"Wake word detected!\")\n",
    "        break\n",
    "\n",
    "audio_stream.close()\n",
    "pa.terminate()\n",
    "\n",
    "# Record and transcribe audio\n",
    "record_audio(duration=2, filename=\"output.wav\")\n",
    "transcription = transcribe_audio(\"output.wav\")\n",
    "print(transcription)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gtts'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgtts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gTTS\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplaysound\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_instrument_status\u001b[39m(image_path):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gtts'"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import requests\n",
    "import os\n",
    "from gtts import gTTS\n",
    "import playsound\n",
    "\n",
    "def get_instrument_status(image_path):\n",
    "    def encode_image(img_path):\n",
    "        with open(img_path, \"rb\") as image_file:\n",
    "            return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "    base64_image = encode_image(image_path)\n",
    "    api_key = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4-vision-preview\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"Is the medical instrument in this image oriented properly and is it sterile?\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
    "                            \"detail\": \"low\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 100\n",
    "    }\n",
    "\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "\n",
    "    try:\n",
    "        response_json = response.json()\n",
    "        instrument_status = response_json.get('choices', [])[0].get('message', {}).get('content', '')\n",
    "        return instrument_status\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "def speak_message(message):\n",
    "    tts = gTTS(text=message, lang='en')\n",
    "    tts.save(\"response.mp3\")\n",
    "    playsound.playsound(\"response.mp3\")\n",
    "    os.remove(\"response.mp3\")\n",
    "\n",
    "# Replace 'path_to_your_image.jpg' with your image path\n",
    "status = get_instrument_status('dirtyScalpel.webp')\n",
    "\n",
    "if \"properly\" in status and \"sterile\" in status:\n",
    "    speak_message(\"MediCarousel is ready\")\n",
    "else:\n",
    "    speak_message(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[0;32m      3\u001b[0m client \u001b[38;5;241m=\u001b[39m OpenAI()\n\u001b[1;32m----> 5\u001b[0m speech_file_path \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;18;43m__file__\u001b[39;49m)\u001b[38;5;241m.\u001b[39mparent \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspeech.mp3\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39maudio\u001b[38;5;241m.\u001b[39mspeech\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m      7\u001b[0m   model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtts-1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      8\u001b[0m   voice\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malloy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      9\u001b[0m   \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToday is a wonderful day to build something people love!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     12\u001b[0m response\u001b[38;5;241m.\u001b[39mstream_to_file(speech_file_path)\n",
      "\u001b[1;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "speech_file_path = Path(__file__).parent / \"speech.mp3\"\n",
    "response = client.audio.speech.create(\n",
    "  model=\"tts-1\",\n",
    "  voice=\"alloy\",\n",
    "  input=\"Today is a wonderful day to build something people love!\"\n",
    ")\n",
    "\n",
    "response.stream_to_file(speech_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aghas\\AppData\\Local\\Temp\\ipykernel_26732\\2821055668.py:11: DeprecationWarning: Due to a bug, this method doesn't actually stream the response content, `.with_streaming_response.method()` should be used instead\n",
      "  response.stream_to_file(\"output.mp3\")\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.audio.speech.create(\n",
    "    model=\"tts-1\",\n",
    "    voice=\"alloy\",\n",
    "    input=\"Hello world! This is a streaming test.\",\n",
    ")\n",
    "\n",
    "response.stream_to_file(\"output.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aghas\\AppData\\Local\\Temp\\ipykernel_29980\\240577385.py:70: DeprecationWarning: Due to a bug, this method doesn't actually stream the response content, `.with_streaming_response.method()` should be used instead\n",
      "  response.stream_to_file(speech_file_path)\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import requests\n",
    "import os\n",
    "from pathlib import Path\n",
    "from openai import OpenAI\n",
    "\n",
    "def get_instrument_status(image_path):\n",
    "    def encode_image(img_path):\n",
    "        with open(img_path, \"rb\") as image_file:\n",
    "            return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "    base64_image = encode_image(image_path)\n",
    "    api_key = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4-vision-preview\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"This is for an educational activity. Briefly identify if the instrument appears to sterile? Answer either: Yes the instrument is sterile, MedCarousel is ready//No, the instrument is not sterile, please clean the instrument\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
    "                            \"detail\": \"med\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 50\n",
    "    }\n",
    "\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "\n",
    "    try:\n",
    "        response_json = response.json()\n",
    "        instrument_status = response_json.get('choices', [])[0].get('message', {}).get('content', '')\n",
    "        return instrument_status\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "def speak_message(message):\n",
    "    client = OpenAI()\n",
    "\n",
    "    # Handling the case where __file__ might not be defined\n",
    "    try:\n",
    "        base_path = Path(__file__).parent\n",
    "    except NameError:\n",
    "        # Fall back to a default directory if __file__ is not defined\n",
    "        base_path = Path.cwd()\n",
    "\n",
    "    speech_file_path = base_path / \"response.mp3\"\n",
    "\n",
    "    response = client.audio.speech.create(\n",
    "      model=\"tts-1\",\n",
    "      voice=\"alloy\",\n",
    "      input=message\n",
    "    )\n",
    "\n",
    "    response.stream_to_file(speech_file_path)\n",
    "\n",
    "    # Play the audio - adjust the command based on your operating system\n",
    "    if os.name == 'nt':  # Windows\n",
    "        os.system(f\"start {speech_file_path}\")\n",
    "    elif os.name == 'posix':  # macOS, Linux, Unix, etc.\n",
    "        os.system(f\"open {speech_file_path}\")\n",
    "\n",
    "status = get_instrument_status('dirtyScalpel.webp')\n",
    "\n",
    "if \"properly\" in status and \"sterile\" in status:\n",
    "    speak_message(\"MediCarousel is ready\")\n",
    "else:\n",
    "    speak_message(status)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ELEC475",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
